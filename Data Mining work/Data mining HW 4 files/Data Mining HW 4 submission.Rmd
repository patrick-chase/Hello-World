---
title: "Data Mining HW 4 Submission"
author: "Patrick Chase"
date: "5/3/2021"
output:
  word_document: default
  pdf_document: default
---
message=FALSE, warning=FALSE, echo=FALSE, error=TRUE

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) 

```



# 1. 
```{r Wine read in, message=FALSE, warning=FALSE, echo=FALSE, error=TRUE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")


library(tidyverse)
library(ggplot2)
library(lubridate)
library(randomForest)
library(splines)
library(pdp)
library(rsample)
library(modelr)
 
wine <- wine %>%
  mutate(color = as.factor(color)) 

wine$is_red = ifelse(wine$color=="red", 1, 0)

wine2 <- subset(wine, select = -c(color))

wine_scaled <- scale(wine2)



```

```{r Principal component identification and data manage, message=FALSE, warning=FALSE, echo=FALSE, error=TRUE}
pca_wine = prcomp(wine[c(1:11)], rank = 30, scale = TRUE)
summary(pca_wine)

wine_combined = data.frame(wine, 
                          pca_wine$x)

wine_split = initial_split(wine_combined, prop = .8)
wine_train = training(wine_split)
wine_test =  testing(wine_split)
```
PCA 1 through PCA 9 capture more than 95% of the variability in our data set. I'll rely on only those for my PCA model. 

```{r PCA models predicting color and quality, message=FALSE, warning=FALSE, echo=FALSE, error=TRUE}

#red model 
pca_glm_red <- glm(is_red ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9, data = wine_train, family = binomial(link = "logit") )

pca_predict_red <- predict(pca_glm_red, wine_test)

## red confusion mtx and rmse 
tab_is_red = table(pca_predict_red > .5, wine_test$is_red)
pca_red_accuracy <- sum(diag(tab_is_red))/sum(tab_is_red)*100
rmse_pca_red <- modelr::rmse(pca_glm_red, wine_test)


# quality model 
pca_lm_qual <- glm(quality ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9, data = wine_train, family = gaussian)

pca_predict_qual <- predict(pca_lm_qual, wine_test)


## quality confusion mtx and rmse 
tab_qual = table(pca_predict_qual > .5, wine_test$quality)
pca_qual_accuracy <- sum(diag(tab_qual))/sum(tab_qual)*100
rmse_pca_qual <- modelr::rmse(pca_lm_qual, wine_test)


```

```{r k-means clustering, message=FALSE, warning=FALSE, echo=FALSE, error=TRUE}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization

test <- subset(wine_scaled, select = c(fixed.acidity, residual.sugar))
testcluster <- kmeans(test, centers = 2, nstart = 25)
red_cluster_viz <- fviz_cluster(testcluster, data = test)
cluster_tab = table(testcluster$cluster, wine$is_red)
cluster_redl_accuracy <- sum(diag(cluster_tab))/sum(cluster_tab)*100

test2 <- subset(wine_scaled, select = c(1:10))
testcluster2 <- kmeans(test2, centers = 10, nstart = 25)
qual_cluster_viz <- fviz_cluster(testcluster2, data = test2)
cluster_tab2 = table(testcluster2$cluster, wine$quality)
cluster_qual_accuracy <- sum(diag(cluster_tab2))/sum(cluster_tab2)*100



```

```{r accuracy data frame, message=FALSE, warning=FALSE, echo=FALSE, error=TRUE}
Accuracy <- data_frame("pca_red_accuracy" = pca_red_accuracy,
                       "rmse_pca_red" = rmse_pca_red, 
                       "pca_qual_accuracy" = pca_qual_accuracy, 
                       "rmse_pca_qual" = rmse_pca_qual, 
                       "cluster_redl_accuracy" = cluster_redl_accuracy, 
                       "cluster_qual_accuracy" = cluster_qual_accuracy)
  
  
  
Accuracy
```

For this specific task, I think PCA makes a lot more sense, particularly for the binary classification between red and white where I can achieve accuracy of 98%. My PCA model estimating quality is just barely wasn't particularly good. As far as I can tell, the K-means models aren't outputting information that is practically useful. I'm not sure if that's because I made a mistake or if there is something else going on. 

# 2. 
```{r data read in , message=FALSE, warning=FALSE, echo=FALSE, error=TRUE}
install.packages("rstatix")
library(rstatix)
library(dplyr)
library(reshape2)
library(ggplot2)

marketing <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv")

marketing.subset <- marketing %>% 
  subset(select = -c(1)) %>%
  data.frame(lapply(marketing, function(x) as.numeric(as.character(x))))

mkt_info <- marketing %>%
  summarize_if(is.numeric, sum, na.rm=TRUE) %>%
  sort(decreasing = TRUE) 

info_conv <- as.data.frame(t(mkt_info)) %>%
  mutate(percent = V1/325802*100)
info_conv$names <- row.names(info_conv)  

info_conv_subset <- info_conv %>% 
  arrange(percent)%>%
  subset(percent >= 3.35) %>%
  subset(names != "chatter")


g1 <- ggplot(data = info_conv) + 
  geom_col(mapping = aes(x = names, y = percent)) +
  theme(axis.text.x = element_text(angle = 90)) +
  ggtitle("Category Percent of Total")+
  theme(plot.title = element_text(hjust = 0.5)) 

g2 <- ggplot(data = info_conv_subset) + 
  geom_col(mapping = aes(x = names, y = percent)) +
  theme(axis.text.x = element_text(angle = 90)) +
  ggtitle("Top 10 Categories minus Chatter")+
  theme(plot.title = element_text(hjust = 0.5)) 



marketing.subset2 <- subset(marketing, select = c(shopping, food, personal_fitness, current_events, college_uni, travel, sports_fandom, politics, cooking, health_nutrition, photo_sharing))





marketing.subset2 <- subset(marketing.subset2, select = -c(X, chatter))
  
cor_mtx <- round(cor(marketing.subset2), 2)



```

```{r g1 , message=FALSE, warning=FALSE, echo=FALSE, error=TRUE}
g1 
```
This first graph is showing the percentage breakdown for each category in our data set. Let's take a closer look by focusing on the top 10 categories removing chatter as it is not substantively useful. 

```{r g2,message=FALSE, warning=FALSE, echo=FALSE, error=TRUE }
g2
```

NutrientH20's social media followers seem to be focused on a typical range of what many consumers on twitter are. That said, a point of interest is the relatively large proportion that is related to food in general, when we aggregate between cooking, food, and health_nutrition. This in combination with the large share of photo_sharing gives us an idea of what some followers may be interested. The market segment in this case would be foodies who enjoy sharing their food endevours with their friends through twitter. This could point to a marketing strategy of engaging with chefs turned social media influencers. This would expand reach and give NutrientH20's consumers what they are interested. 



# 3. 


```{r 3 read in and rule generation, message=FALSE, warning=FALSE, echo=FALSE, error=TRUE}
library(tidyverse)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
library(igraph)
library(widyr)
library(ggraph)
library(readr)
library(data.table)
library(stringr)

groceries_raw <- read.transactions("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/groceries.txt", format = c("basket"), rm.duplicates = TRUE)



grocery_rules = apriori(groceries_raw, 
                        parameter=list(support=.005, confidence=.15, minlen = 2, maxlen=2))
plot(grocery_rules, 
     measure = c("support", "lift"),
     shading = "confidence")




```
I chose a low level for support because I don't think a grocer would be particularly interested in focusing on products that all of their customers already buy. My threshhold for confidence was 15% because I think it would be more useufl to show associations of products in order to inform store organization. I decided to subset my rules to focus on those with the highest confidence in the graphic below. 


```{r more arule plots, message=FALSE, warning=FALSE, echo=FALSE, error=TRUE}

subrules <- head(sort(grocery_rules, by = "confidence"), 30)
plot.2 <- plot(subrules, method = "graph", control = list(type="items"))

```
This graphic shows rules that seem to follow the typical organization of a grocery store. Dairy products are largely purchased in tandem with each other, and most customers seem to be getting whole vegetables along with many other products. 


# 4. 
```{r corpus construction code ,message=FALSE, warning=FALSE, echo=FALSE, error=TRUE }
library(tidyverse)
library(tm)
library(gamlr)
library(SnowballC)
library("quanteda")
library(methods)

readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }


train_dirs = Sys.glob('/Users/patrickchase/Desktop/GitHub/ECO395M/data/ReutersC50/C50train/*')
train_dirs = train_dirs[c(43, 47)]
file_list = NULL
labels_train = NULL
for(author in train_dirs) {
	author_name = substring(author, first=29)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list = append(file_list, files_to_add)
	labels_train = append(labels_train, rep(author_name, length(files_to_add)))
}

corpus_train = Corpus(DirSource(train_dirs)) 

corpus_train = corpus_train %>% tm_map(., content_transformer(tolower)) %>% 
        tm_map(., content_transformer(removeNumbers)) %>% 
				tm_map(., content_transformer(removeNumbers)) %>% 
				tm_map(., content_transformer(removePunctuation)) %>%
				tm_map(., content_transformer(stripWhitespace)) %>%
				tm_map(., content_transformer(removeWords), stopwords("SMART"))



test_dirs = Sys.glob('~/Desktop/GitHub/ECO395M/data/ReutersC50/C50test/*')
test_dirs = test_dirs[c(43, 47)]
file_list = NULL
labels_test = NULL
for(author in test_dirs) {
	author_name = substring(author, first=28)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list = append(file_list, files_to_add)
	labels_test = append(labels_test, rep(author_name, length(files_to_add)))
}


corpus_test = Corpus(DirSource(test_dirs)) 

corpus_test = corpus_test %>% tm_map(., content_transformer(tolower)) %>% 
				tm_map(., content_transformer(removeNumbers)) %>% 
				tm_map(., content_transformer(removePunctuation)) %>%
				tm_map(., content_transformer(stripWhitespace)) %>%
				tm_map(., content_transformer(removeWords), stopwords("SMART")) 



```


For my data cleaning procedure I basically followed you're work exactly. And even with that I struggled to transition it to a usable format where I could use the principal components I identified to build a classification model. That said, I had intended to build a model where I'd focus on classifying the work of one Mure Dickie. I imported the Document Term Matrix into a normal matrix and normalized it. I had intended to use my PCs as variables in a logit model, with the outcome variable being Author == "MureDickie". I suspect my classification may have been alright, although probably not great. My issue, ultimately was a data processing one. I was ultimately able to get my model to run, but I have no idea what the accuracy of it is. 

```{r, message=FALSE, warning=FALSE, echo=FALSE, error=TRUE}

library(irlba)
library(tidyverse)
library(scales)
library(broom)
library(glmnet)
library(dplyr)
library(tidyr)
library(caret)
library(nnet)


DTM_train = DocumentTermMatrix(corpus_train)

DTM_test = DocumentTermMatrix(corpus_test, control = list(dictionary=Terms(DTM_train)))


dtm_train2 <- as.matrix(DTM_train)
dtm_test2 <- as.matrix(DTM_test)



dtm_norm_train <- t(apply(dtm_train2, 1, function(x) x/sqrt(sum(x^2))))
dtm_norm_test <- t(apply(DTM_test, 1, function(x) x/sqrt(sum(x^2))))

pca_identify <- prcomp_irlba(dtm_norm_train, n=90)
pca_actual <- prcomp_irlba(dtm_norm_train, n=64, scale = TRUE)



pca_in <- data.frame(pca_actual$x)
pca_in <- na.omit(pca_in)

y_train = 0 + {labels_train=='MureDickie'}

#model specification 
glm1 = glm(y_train ~ . , data = pca_in, family = "binomial" )


rmse_glm1 <- modelr::rmse(glm1, dtm_norm_test)

```

